<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta
      name="description"
      content="BiFold predicts bimanual cloth folding actions."
    />
    <meta name="keywords" content="Cloth, LLM, Robotics, VLM, LMM" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>BiFold</title>

    <link
      href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
      rel="stylesheet"
    />

    <link rel="stylesheet" href="./static/css/bulma.min.css" />
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css" />
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css" />
    <script
      src="https://kit.fontawesome.com/e038779150.js"
      crossorigin="anonymous"
    ></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>

    <link rel="stylesheet" href="../css/academicons.min.css" />

    <script
      defer
      src="https://use.fontawesome.com/releases/v5.3.1/js/all.js"
    ></script>
    <link rel="stylesheet" href="./static/css/index.css" />
    <link rel="icon" href="../img/webico.ico" />

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <script language="JavaScript">
      function ShowHide(divId) {
        $(divId).slideToggle(200);
      }
    </script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.11/clipboard.min.js"></script>
  </head>
  <body>
    <style>
      .grad_text {
        background: -webkit-linear-gradient(right, #ffa300, #772583);
        -webkit-background-clip: text;
        background-clip: text;
        -webkit-text-fill-color: transparent;
      }
    </style>

    <nav
      class="navbar is-transparent is-fixed-top glass-overlay"
      role="navigation"
      aria-label="main navigation"
    >
      <div class="navbar-brand">
        <a
          role="button"
          class="navbar-burger"
          aria-label="menu"
          aria-expanded="false"
        >
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
        </a>
      </div>
      <div class="navbar-menu">
        <a class="navbar-item" href="#">
          <span class="icon">
            <i class="fas fa-arrow-up"></i>
          </span>
        </a>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#additional-evaluations">
            Aditional evaluations
          </a>
          <div class="navbar-dropdown is-boxed">
            <a class="navbar-item" href="#unimanual-folding">
              Unimanual folding
            </a>
            <a class="navbar-item" href="#bimanual-folding">
              Bimanual folding
            </a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#dataset_generation">
            Dataset generation
          </a>
          <div class="navbar-dropdown is-boxed">
            <a class="navbar-item" href="#rendering"> Rendering </a>
            <a class="navbar-item" href="#language_annotations">
              Language annotations
            </a>
            <a class="navbar-item" href="#filtering">
              Filtering out divergent sequences
            </a>
            <a class="navbar-item" href="#statistics"> Statistics </a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#experimental_setup">
            Experimental setup
          </a>
          <div class="navbar-dropdown is-boxed">
            <a class="navbar-item" href="#modeling"> Modeling </a>
            <a class="navbar-item" href="#simulation"> Simulation </a>
            <a class="navbar-item" href="#real"> Real </a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#additional_experiments">
            Additional experiments
          </a>
          <div class="navbar-dropdown is-boxed">
            <a class="navbar-item" href="#qualitative">
              Qualitative evaluation
            </a>
            <a class="navbar-item" href="#quantitative">
              Quantitative evaluation
            </a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#limitations"> Limitations </a>
          <div class="navbar-dropdown is-boxed">
            <a class="navbar-item" href="#related_work_limitations">
              Previous approaches
            </a>
            <a class="navbar-item" href="#dataset_extensions">
              BiFold dataset
            </a>
            <a class="navbar-item" href="#modeling_and_evaluation">
              BiFold model and evaluation
            </a>
          </div>
        </div>
        <div class="navbar-end" style="flex-grow: 1">
          <a class="navbar-item" href="../">
            <span class="icon">
              <i class="fas fa-home"></i>
            </span>
          </a>

          <div class="navbar-item has-dropdown is-hoverable">
            <a class="navbar-link"> Related Research </a>
            <div class="navbar-dropdown is-right-aligned">
              <a
                class="navbar-item"
                href="https://sites.google.com/view/cloth-sim2real-benchmark"
              >
                Cloth Sim2Real Benchmark
              </a>
            </div>
          </div>
        </div>
      </div>
    </nav>

    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">
                <br />
                <span class="grad_text">BiFold</span>: Bimanual Cloth Folding
                with Language Guidance
              </h1>
              <h2 class="title is-3">ICRA 2025</h2>
              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a href="../">Oriol Barbany</a><sup>1</sup>,</span
                >
                <span class="author-block">
                  <a href="https://www.iri.upc.edu/staff/acolome"
                    >Adrià Colomé</a
                  ><sup>1</sup>,</span
                >
                <span class="author-block">
                  <a href="https://www.iri.upc.edu/people/torras/"
                    >Carme Torras</a
                  ><sup>1</sup>,
                </span>
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block"
                  ><sup>1</sup>Institut de Robòtica i Informàtica Industrial,
                  CSIC-UPC</span
                >
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- PDF Link. -->
                  <!-- <span class="link-block">
                    <a
                      href="https://arxiv.org/pdf/2011.12948"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span> -->
                  <span class="link-block">
                    <a
                      href="https://arxiv.org/abs/2501.16458"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>BiFold</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a
                      href="https://arxiv.org/abs/2505.07600"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>Workshop extension</span>
                    </a>
                  </span>
                  <!-- Video Link. -->
                  <!-- <span class="link-block"> -->
                  <!--   <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA" -->
                  <!--      class="external-link button is-normal is-rounded is-dark"> -->
                  <!--     <span class="icon"> -->
                  <!--         <i class="fab fa-youtube"></i> -->
                  <!--     </span> -->
                  <!--     <span>Video</span> -->
                  <!--   </a> -->
                  <!-- </span> -->
                  <!-- Code Link. -->
                  <span class="link-block">
                    <a
                      href="https://github.com/Barbany/bifold"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>
                  <!-- Dataset Link. -->
                  <span class="link-block">
                    <a
                      href="https://zenodo.org/records/14851100"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fa-solid fa-database"></i>
                      </span>
                      <span>Data</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a
                      href="https://x.com/oriolbarbany/status/1890428992978645198"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <i class="fa-brands fa-x-twitter"></i>Thread
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section">
      <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                Cloth folding is a complex task due to the inevitable
                self-occlusions of clothes, their complicated dynamics, and the
                disparate materials, geometries, and textures that garments can
                have. In this work, we learn folding actions conditioned on text
                commands. Translating high-level, abstract instructions into
                precise robotic actions requires sophisticated language
                understanding and manipulation capabilities. To do that, we
                leverage a pre-trained vision-language model and repurpose it to
                predict manipulation actions. Our model, BiFold, can take
                context into account and achieves state-of-the-art performance
                on an existing language-conditioned folding benchmark. To
                address the lack of annotated bimanual folding data, we
                introduce a novel dataset with automatically parsed actions and
                language-aligned instructions, enabling better learning of
                text-conditioned manipulation. BiFold attains the best
                performance on our dataset and demonstrates strong
                generalization to new instructions, garments, and environments.
              </p>
              <p>The main contributions of this work are:</p>
              <ul>
                <li>
                  A new model that leverages a foundational vision-language
                  backend to learn cloth folding manipulations taking previous
                  actions into account.
                </li>
                <li>
                  A novel dataset of language-aligned bimanual cloth folding
                  actions obtained with an automatic pipeline that could be
                  applied to other datasets
                </li>
                <li>
                  We demonstrate the superiority of our model on an existing
                  benchmark of unimodal manipulation, the newly introduced
                  bimanual dataset, and on a real-world setup.
                </li>
              </ul>
            </div>
          </div>
        </div>

        <h2 class="title is-3">Papers</h2>
        <!-- Paper 1: Use box + is-primary -->
        <div
          class="box has-text-centered"
          style="
            background: -webkit-linear-gradient(right, #ffa2002b, #77258329);
          "
        >
          <h3 class="title is-3">
            BiFold: Bimanual Cloth Folding with Language Guidance
          </h3>
          <h3 class="title is-4">ICRA 2025</h3>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="../">Oriol Barbany</a><sup>1</sup>,</span
            >
            <span class="author-block">
              <a href="https://www.iri.upc.edu/staff/acolome">Adrià Colomé</a
              ><sup>1</sup>,</span
            >
            <span class="author-block">
              <a href="https://www.iri.upc.edu/people/torras/">Carme Torras</a
              ><sup>1</sup>,
            </span>
          </div>
          <br />
          <p>
            Proposes a vision-language model (BiFold) for bimanual cloth folding
            conditioned on natural language instructions. It combines LoRA-tuned
            SigLIP encoders with a transformer to predict pick-and-place actions
            from keyframes, enabling end-to-end learning without explicit cloth
            representations. The model is trained on a new dataset annotated
            through a scalable, automatic procedure for generating
            language-aligned instructions.
          </p>
          <br />
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a
                  href="https://arxiv.org/abs/2501.16458"
                  target="_blank"
                  class="external-link button is-normal is-rounded is-dark"
                >
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a
                  href="https://github.com/barbany/bifold"
                  target="_blank"
                  class="external-link button is-normal is-rounded is-dark"
                >
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              <span class="link-block">
                <a
                  href="https://zenodo.org/records/14851100"
                  class="external-link button is-normal is-rounded is-dark"
                >
                  <span class="icon">
                    <i class="fa-solid fa-database"></i>
                  </span>
                  <span>Data</span>
                </a>
              </span>
              <!-- Poster -->
              <span class="link-block">
                <a
                  href="media/icra_poster.pdf"
                  target="_blank"
                  class="external-link button is-normal is-rounded is-dark"
                >
                  <span class="icon">
                    <i class="far fa-sticky-note"></i>
                  </span>
                  <span>Poster</span>
                </a>
              </span>
              <!-- Slides (Keynote) -->
              <span class="link-block">
                <a
                  href="media/icra.key"
                  target="_blank"
                  class="external-link button is-normal is-rounded is-dark"
                >
                  <span class="icon">
                    <i class="fas fa-file-powerpoint"></i>
                  </span>
                  <span>Slides (Keynote)</span>
                </a>
              </span>

              <!-- BibTeX -->
              <span class="link-block">
                <a
                  class="external-link button is-normal is-rounded is-dark js-modal-trigger"
                  onclick="javascript:ShowHide('#bibtex_bifold')"
                  href="javascript:;"
                  style="text-decoration: none"
                >
                  <i class="fa-sharp fa-solid fa-link"></i>
                  BibTeX
                </a>
              </span>
            </div>
          </div>
          <pre id="bibtex_bifold" style="display: none; text-align: left">
@inproceedings{bifold,
  title={{BiFold: Bimanual Cloth Folding with Language Guidance}},
  author={Oriol Barbany and Adrià Colomé and Carme Torras},
  booktitle={IEEE International Conference on Robotics and Automation},
  year={2025}
}</pre
          >
          <br />
          <h2 class="title is-4">
            Video (3 min) <i class="fa fa-volume-up"></i>
          </h2>
          <video controls preload>
            <source src="media/bifold_icra25.mp4" , type="video/mp4" />
          </video>
        </div>

        <!-- Paper 2: Use box + is-link -->
        <div
          class="box has-text-centered"
          style="
            background: -webkit-linear-gradient(right, #ffa2002b, #77258329);
          "
        >
          <h3 class="title is-3">
            Beyond Static Perception: Integrating Temporal Context into VLMs for
            Cloth Folding
          </h3>
          <h3 class="title is-4">
            ICRA 2025 Workshop
            <a href="https://deformable-workshop.github.io/icra2025/"
              >"Reflections on Representations and Manipulating Deformable
              Objects"</a
            >
          </h3>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="../">Oriol Barbany</a><sup>1</sup>,</span
            >
            <span class="author-block">
              <a href="https://www.iri.upc.edu/staff/acolome">Adrià Colomé</a
              ><sup>1</sup>,</span
            >
            <span class="author-block">
              <a href="https://www.iri.upc.edu/people/torras/">Carme Torras</a
              ><sup>1</sup>,
            </span>
          </div>
          <br />
          <p>
            Analyzes the BiFold model to understand how temporal context and
            fine-tuning influence perception. Shows that using keyframes
            improves feature consistency, attention alignment, and state
            recognition compared to static or redundant frame inputs.
          </p>
          <br />
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a
                  href="https://arxiv.org/abs/2505.07600"
                  target="_blank"
                  class="external-link button is-normal is-rounded is-dark"
                >
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a
                  href="https://github.com/barbany/bifold"
                  target="_blank"
                  class="external-link button is-normal is-rounded is-dark"
                >
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              <!-- Poster -->
              <span class="link-block">
                <a
                  href="media/icraw_poster.pdf"
                  target="_blank"
                  class="external-link button is-normal is-rounded is-dark"
                >
                  <span class="icon">
                    <i class="far fa-sticky-note"></i>
                  </span>
                  <span>Poster</span>
                </a>
              </span>
              <!-- Slides (Keynote) -->
              <span class="link-block">
                <a
                  href="media/icraw.key"
                  target="_blank"
                  class="external-link button is-normal is-rounded is-dark"
                >
                  <span class="icon">
                    <i class="fas fa-file-powerpoint"></i>
                  </span>
                  <span>Slides (Keynote)</span>
                </a>
              </span>

              <!-- BibTeX -->
              <span class="link-block">
                <a
                  class="external-link button is-normal is-rounded is-dark js-modal-trigger"
                  onclick="javascript:ShowHide('#bibtex_temporal')"
                  href="javascript:;"
                  style="text-decoration: none"
                >
                  <i class="fa-sharp fa-solid fa-link"></i>
                  BibTeX
                </a>
              </span>
            </div>
          </div>
          <pre id="bibtex_temporal" style="display: none; text-align: left">
@misc{barbany25temporalcontext,
  title={{Beyond Static Perception: Integrating Temporal Context into VLMs for Cloth Folding}},
  author={Oriol Barbany and Adrià Colomé and Carme Torras},
  year={2025},
  eprint={2505.07600},
  archivePrefix={arXiv},
  primaryClass={cs.RO},
  url={https://arxiv.org/abs/2505.07600}, 
}</pre
          >
          <br />
          <h2 class="title is-4">
            Video (3 min) <i class="fa fa-volume-up"></i>
          </h2>
          <video controls preload>
            <source src="media/video_icraw.mp4" , type="video/mp4" />
          </video>
        </div>
      </div>
    </section>

    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3" id="additional-evaluations">
              Additional evaluations
            </h2>
            <h3 class="title is-4" id="unimanual-folding">Unimanual folding</h3>
            <div class="content has-text-justified">
              <p>
                Below we present some of the actions predicted by BiFold on the
                unimanual dataset. You can hover over the images to see the text
                instruction.
              </p>
            </div>
          </div>
        </div>
        <div id="unimanual-carousel" class="carousel results-carousel">
          <div class="item">
            <img
              src="./media/unimanual_actions/ut_8_1_Make a fold from the bottommost right corner of the fabric to the top left-hand._True.png"
              class="image"
            />
            <div class="overlay">
              [UT] Make a fold from the bottommost right corner of the fabric to
              the top left-hand
            </div>
          </div>
          <div class="item">
            <img
              src="./media/unimanual_actions/usi_3_0_Fold the right-hand sleeve to the centerline of the shirt._True.png"
              class="image"
            />
            <div class="overlay">
              [USI] Fold the right-hand sleeve to the centerline of the shirt
            </div>
          </div>
          <div class="item">
            <img
              src="./media/unimanual_actions/ut_33_0_Fold the Trousers in half, with the rightmost side overlapping the leftmost._True.png"
              class="image"
            />
            <div class="overlay">
              [UT] Fold the Trousers in half, with the rightmost side
              overlapping the leftmost
            </div>
          </div>
          <div class="item">
            <img
              src="./media/unimanual_actions/usi_47_1_Fold the Trousers, making a crease from the leftmost to the rightmost._True.png"
              class="image"
            />
            <div class="overlay">
              [USI] Fold the Trousers, making a crease from the leftmost to the
              rightmost
            </div>
          </div>
          <div class="item">
            <img
              src="./media/unimanual_actions/si_46_0_Make a fold in the Trousers, starting from the left-hand and ending at the rightmost._True.png"
              class="image"
            />
            <div class="overlay">
              [SI] Make a fold in the Trousers, starting from the left-hand and
              ending at the rightmost
            </div>
          </div>
          <div class="item">
            <img
              src="./media/unimanual_actions/si_37_2_Fold the Trousers by bringing the waistband down to meet the bottom._True.png"
              class="image"
            />
            <div class="overlay">
              [SI] Fold the Trousers by bringing the waistband down to meet the
              bottom
            </div>
          </div>
          <div class="item">
            <img
              src="./media/unimanual_actions/usi_19_1_Fold the cloth in half, starting from the leftmost side and meeting the right._True.png"
              class="image"
            />
            <div class="overlay">
              [USI] Fold the cloth in half, starting from the leftmost side and
              meeting the right
            </div>
          </div>
          <div class="item">
            <img
              src="./media/unimanual_actions/usi_8_0_Make a fold in the cloth by halving it from top to lowermost._True.png"
              class="image"
            />
            <div class="overlay">
              [USI] Make a fold in the cloth by halving it from top to lowermost
            </div>
          </div>
          <div class="item">
            <img
              src="./media/unimanual_actions/usi_13_1_Bring the bottommost left corner of the cloth down to the right upper corner, folding it in half diagonally._True.png"
              class="image"
            />
            <div class="overlay">
              [USI] Bring the bottommost left corner of the cloth down to the
              right upper corner, folding it in half diagonally
            </div>
          </div>
          <div class="item">
            <img
              src="./media/unimanual_actions/ut_35_1_Fold the left-hand sleeve towards the inside._True.png"
              class="image"
            />
            <div class="overlay">
              [UT] Fold the left-hand sleeve towards the inside
            </div>
          </div>
          <div class="item">
            <img
              src="./media/unimanual_actions/si_0_2_Flip the bottom of the T-shirt towards the top._True.png"
              class="image"
            />
            <div class="overlay">
              [SI] Flip the bottom of the T-shirt towards the top
            </div>
          </div>
          <div class="item">
            <img
              src="./media/unimanual_actions/usi_9_0_Make a crease at the leftmost top corner of the cloth and fold it towards the center._True.png"
              class="image"
            />
            <div class="overlay">
              [USI] Make a crease at the leftmost top corner of the cloth and
              fold it towards the center
            </div>
          </div>
          <div class="item">
            <img
              src="./media/unimanual_actions/si_1_3_Bring the bottom left-hand corner of the cloth to the middle with a fold._True.png"
              class="image"
            />
            <div class="overlay">
              [SI] Bring the bottom left-hand corner of the cloth to the middle
              with a fold
            </div>
          </div>
          <div class="item">
            <img
              src="./media/unimanual_actions/si_14_0_Take the left-hand bottom corner of the material and fold it to the corner on the opposite side._True.png"
              class="image"
            />
            <div class="overlay">
              [SI] Take the left-hand bottom corner of the material and fold it
              to the corner on the opposite side
            </div>
          </div>
          <div class="item">
            <img
              src="./media/unimanual_actions/usi_15_2_Make a crease at the leftmost top corner of the cloth and fold it towards the center._True.png"
              class="image"
            />
            <div class="overlay">
              [USI] Make a crease at the leftmost top corner of the cloth and
              fold it towards the center
            </div>
          </div>
          <div class="item">
            <img
              src="./media/unimanual_actions/si_4_0_Create a central fold in the cloth by folding it in half from uppermost to lower._True.png"
              class="image"
            />
            <div class="overlay">
              [SI] Create a central fold in the cloth by folding it in half from
              uppermost to lower
            </div>
          </div>
          <div class="item">
            <img
              src="./media/unimanual_actions/si_46_1_Fold the Trousers from the left-hand side towards the right side._True.png"
              class="image"
            />
            <div class="overlay">
              [SI] Fold the Trousers from the left-hand side towards the right
              side
            </div>
          </div>
          <div class="item">
            <img
              src="./media/unimanual_actions/si_5_0_Create a fold in the cloth by halving it from uppermost to lower._True.png"
              class="image"
            />
            <div class="overlay">
              [SI] Create a fold in the cloth by halving it from uppermost to
              lower
            </div>
          </div>
          <div class="item">
            <img
              src="./media/unimanual_actions/si_6_2_Fold the fabric in half, starting from the left side._True.png"
              class="image"
            />
            <div class="overlay">
              [SI] Fold the fabric in half, starting from the left side
            </div>
          </div>
          <div class="item">
            <img
              src="./media/unimanual_actions/ut_46_2_Fold the textile symmetrically, starting on the bottom._True.png"
              class="image"
            />
            <div class="overlay">
              [UT] Fold the textile symmetrically, starting on the bottom
            </div>
          </div>
          <div class="item">
            <img
              src="./media/unimanual_actions/si_0_3_Tuck the bottom of the T-shirt upwards._True.png"
              class="image"
            />
            <div class="overlay">
              [SI] Tuck the bottom of the T-shirt upwards
            </div>
          </div>
          <div class="item">
            <img
              src="./media/unimanual_actions/ut_20_2_Create a fold from the lower right corner of the cloth towards the center._True.png"
              class="image"
            />
            <div class="overlay">
              [UT] Create a fold from the lower right corner of the cloth
              towards the center
            </div>
          </div>
          <div class="item">
            <img
              src="./media/unimanual_actions/ut_8_0_Fold the rightmost top corner of the fabric to its diagonal corner._True.png"
              class="image"
            />
            <div class="overlay">
              [UT] Fold the rightmost top corner of the fabric to its diagonal
              corner
            </div>
          </div>
        </div>
        <br />
        <div class="content has-text-justified">
          <p>
            While our model predicts a single action, we can concatenate some
            predefined actions to perform an end-to-end fold:
          </p>
          <div class="columns is-vcentered">
            <div class="column is-one-quarter">
              <img src="./media/unimanual_gifs/0.gif" />
            </div>
            <div class="column is-one-quarter">
              <img src="./media/unimanual_gifs/1.gif" />
            </div>
            <div class="column is-one-quarter">
              <img src="./media/unimanual_gifs/2.gif" />
            </div>
            <div class="column is-one-quarter">
              <img src="./media/unimanual_gifs/3.gif" />
            </div>
          </div>
        </div>
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h3 class="title is-4" id="bimanual-folding">Bimanual folding</h3>
            <p>
              We include action visualizations for the different bimanual
              folding environments. In all the following qualitative examples,
              the pick and place actions for the right and left grippers are
              represented as the origin and endpoints of arrows:
              <strong><font color="#FF0000">red</font></strong>
              and
              <strong><font color="#00FF00">green</font></strong>
              for ground truth (included if using our dataset), and
              <strong><font color="#0000FF">blue</font></strong>
              and
              <strong><font color="#18EDF2">light blue</font></strong>
              for our model, respectively.
            </p>
            <br />

            <h3 class="title is-5">Test samples of our dataset</h3>
            <div class="content has-text-justified">
              <p>
                Below are some examples of action predictions on the test
                partition of our dataset. Note that here the evaluation is
                performed on static images, so we report image-based metrics in
                the paper.
              </p>
            </div>
          </div>
        </div>
        <div id="bimanual-dataset-carousel" class="carousel results-carousel">
          <div class="item">
            <img
              src="./media/bimanual_actions_dataset/599_Fold the skirt, bringing the top side to meet the bottom side..png"
              class="image"
            />
            <div class="overlay">
              599_Fold the skirt, bringing the top side to meet the bottom side.
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_dataset/34_Fold the skirt neatly, from the right side to the left side..png"
              class="image"
            />
            <div class="overlay">
              34_Fold the skirt neatly, from the right side to the left side.
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_dataset/514_Fold the trousers, orientating from the right towards the left..png"
              class="image"
            />
            <div class="overlay">
              514_Fold the trousers, orientating from the right towards the
              left.
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_dataset/51_Fold the tshirt, making a crease from the right to the left..png"
              class="image"
            />
            <div class="overlay">
              51_Fold the tshirt, making a crease from the right to the left.
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_dataset/368_Fold the trousers, bottom left side over bottom right side..png"
              class="image"
            />
            <div class="overlay">
              368_Fold the trousers, bottom left side over bottom right side.
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_dataset/222_Bring the bottom side of the top towards the top side and fold them in half..png"
              class="image"
            />
            <div class="overlay">
              222_Bring the bottom side of the top towards the top side and fold
              them in half.
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_dataset/42_Fold the skirt in half, aligning the right and left sides..png"
              class="image"
            />
            <div class="overlay">
              42_Fold the skirt in half, aligning the right and left sides.
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_dataset/414_Fold the trousers in half, with the top side overlapping the bottom..png"
              class="image"
            />
            <div class="overlay">
              414_Fold the trousers in half, with the top side overlapping the
              bottom.
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_dataset/231_Make a fold in the top, starting from the bottom and ending at the top..png"
              class="image"
            />
            <div class="overlay">
              231_Make a fold in the top, starting from the bottom and ending at
              the top.
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_dataset/206_Fold the left sleeve to the centerline of the shirt..png"
              class="image"
            />
            <div class="overlay">
              206_Fold the left sleeve to the centerline of the shirt.
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_dataset/15_Fold the skirt cleanly, from the right side to the left side..png"
              class="image"
            />
            <div class="overlay">
              15_Fold the skirt cleanly, from the right side to the left side.
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_dataset/55_Fold the left sleeve inward to the halfway point..png"
              class="image"
            />
            <div class="overlay">
              55_Fold the left sleeve inward to the halfway point.
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_dataset/61_Fold the tshirt in half, with the bottom right side overlapping the bottom left only using the right arm..png"
              class="image"
            />
            <div class="overlay">
              61_Fold the tshirt in half, with the bottom right side overlapping
              the bottom left only using the right arm.
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_dataset/318_Fold the top, making sure the bottom side touches the top side..png"
              class="image"
            />
            <div class="overlay">
              318_Fold the top, making sure the bottom side touches the top
              side.
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_dataset/306_Fold the top neatly, from the right side to the bottom side..png"
              class="image"
            />
            <div class="overlay">
              306_Fold the top neatly, from the right side to the bottom side.
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_dataset/24_Bend the skirt in half, from right to left..png"
              class="image"
            />
            <div class="overlay">
              24_Bend the skirt in half, from right to left.
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_dataset/186_Fold the right sleeve towards the body only using the right arm..png"
              class="image"
            />
            <div class="overlay">
              186_Fold the right sleeve towards the body only using the right
              arm.
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_dataset/367_Fold the trousers in half, starting from the left and ending at the right..png"
              class="image"
            />
            <div class="overlay">
              367_Fold the trousers in half, starting from the left and ending
              at the right.
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_dataset/253_Fold the top, making a crease from the right to the left..png"
              class="image"
            />
            <div class="overlay">
              253_Fold the top, making a crease from the right to the left.
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_dataset/496_Fold the trousers in half, bottom right to left..png"
              class="image"
            />
            <div class="overlay">
              496_Fold the trousers in half, bottom right to left.
            </div>
          </div>
        </div>
        <br />
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h3 class="title is-5">SoftGym evaluation</h3>
            <div class="content has-text-justified">
              <p>
                To evaluate the manipulation performance, we load the meshes in
                SoftGym and evaluate the model without training. Note that there
                is an evident distribution shift between the created dataset and
                the SoftGym appearance and BiFold has not seen any observation
                coming from SoftGym. Here are some of the actions performed in
                SoftGym:
              </p>
            </div>
            <div class="columns is-vcentered">
              <div class="column is-one-quarter">
                <img src="./media/bimanual_gifs/fold_pants.gif" />
              </div>
              <div class="column is-one-quarter">
                <img src="./media/bimanual_gifs/fold_shirt.gif" />
              </div>
              <div class="column is-one-quarter">
                <img src="./media/bimanual_gifs/fold_skirt.gif" />
              </div>
              <div class="column is-one-quarter">
                <img src="./media/bimanual_gifs/fold_top.gif" />
              </div>
            </div>
            <div class="content has-text-justified">
              <p>
                Similarly to before, we provide some of BiFold's predictions:
              </p>
            </div>
          </div>
        </div>
        <div id="bimanual-softgym-carousel" class="carousel results-carousel">
          <div class="item">
            <img
              src="./media/bimanual_actions_softgym/01830_Trousers_000371_000035_Fold the trousers in half horizontally, top left to right._True.png"
              class="image"
            />
            <div class="overlay">
              Fold the trousers in half horizontally, top left to right
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_softgym/01272_Trousers_000256_000035_Fold the trousers neatly, from the left side to the right side._True.png"
              class="image"
            />
            <div class="overlay">
              Fold the trousers neatly, from the left side to the right side
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_softgym/01532_Top_000193_000045_Fold the top in half, starting from the left and ending at the right._True.png"
              class="image"
            />
            <div class="overlay">
              Fold the top in half, starting from the left and ending at the
              right
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_softgym/03912_Skirt_000240_000030_Fold the skirt cleanly, from the left side to the right side._True.png"
              class="image"
            />
            <div class="overlay">
              Fold the skirt cleanly, from the left side to the right side
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_softgym/01681_Trousers_000353_000025_Make a fold in the trousers, starting from the right and ending at the left._True.png"
              class="image"
            />
            <div class="overlay">
              Make a fold in the trousers, starting from the right and ending at
              the left
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_softgym/00773_Tshirt_000102_000215_Make a fold in the tshirt, starting from the bottom left and ending at the top._True.png"
              class="image"
            />
            <div class="overlay">
              Make a fold in the tshirt, starting from the bottom left and
              ending at the top
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_softgym/02796_Tshirt_000355_000215_Fold the right sleeve to the centerline of the shirt._True.png"
              class="image"
            />
            <div class="overlay">
              Fold the right sleeve to the centerline of the shirt
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_softgym/01552_Top_000194_000165_Fold the top neatly, from the bottom left side to the bottom right side._True.png"
              class="image"
            />
            <div class="overlay">
              Fold the top neatly, from the bottom left side to the bottom right
              side
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_softgym/03132_Tshirt_000389_000030_Fold the right sleeve inward to the halfway point._True.png"
              class="image"
            />
            <div class="overlay">
              Fold the right sleeve inward to the halfway point
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_softgym/06971_Top_000830_000035_Fold the top, orientating from the right towards the left._True.png"
              class="image"
            />
            <div class="overlay">
              Fold the top, orientating from the right towards the left
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_softgym/01700_Tshirt_000234_000025_Bring the left sleeve to the center._True.png"
              class="image"
            />
            <div class="overlay">Bring the left sleeve to the center</div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_softgym/02363_Trousers_000459_000115_Fold the trousers, left side over right side._True.png"
              class="image"
            />
            <div class="overlay">
              Fold the trousers, left side over right side
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_softgym/01285_Tshirt_000168_000135_Fold the tshirt, left side over right side._True.png"
              class="image"
            />
            <div class="overlay">
              Fold the tshirt, left side over right side
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_softgym/04523_Trousers_000925_000155_Bend the trousers in half, from bottom left to bottom right._True.png"
              class="image"
            />
            <div class="overlay">
              Bend the trousers in half, from bottom left to bottom right
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_softgym/07017_Trousers_001415_000040_Fold the trousers, making a crease from the left to the right._True.png"
              class="image"
            />
            <div class="overlay">
              Fold the trousers, making a crease from the left to the right
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_softgym/03943_Tshirt_000496_000115_Fold the tshirt cleanly, from the bottom side to the top side._True.png"
              class="image"
            />
            <div class="overlay">
              Fold the tshirt cleanly, from the bottom side to the top side
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_softgym/00380_Tshirt_000051_000055_Fold the left sleeve towards the midpoint of the shirt._True.png"
              class="image"
            />
            <div class="overlay">
              Fold the left sleeve towards the midpoint of the shirt
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_softgym/02161_Skirt_000149_000040_Bend the skirt in half, from right to left._True.png"
              class="image"
            />
            <div class="overlay">
              Bend the skirt in half, from right to left
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_softgym/01988_Tshirt_000262_000175_Fold the tshirt from the left side towards the right side._True.png"
              class="image"
            />
            <div class="overlay">
              Fold the tshirt from the left side towards the right side
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_softgym/07639_Tshirt_000990_000155_Fold the right sleeve to the center._True.png"
              class="image"
            />
            <div class="overlay">Fold the right sleeve to the center</div>
          </div>
        </div>
        <br />
        <div class="columns is-centered">
          <div class="column is-full-width">
            <div class="content has-text-justified">
              <p>
                As with the unimanual dataset, we can also easily perform full
                rollouts by concatenating folding actions:
              </p>
            </div>
            <div class="columns is-vcentered">
              <div class="column is-one-third">
                <img src="./media/bimanual_samples/0.gif" />
              </div>
              <div class="column is-one-third">
                <img src="./media/bimanual_samples/1.gif" />
              </div>
              <div class="column is-one-third">
                <img src="./media/bimanual_samples/3.gif" />
              </div>
            </div>
            <h3 class="title is-5">Real evaluation</h3>
            <div class="content has-text-justified">
              <p>
                We also test our model on real images without any adaptation or
                fine-tuning:
              </p>
            </div>
          </div>
        </div>
        <div id="bimanual-real-carousel" class="carousel results-carousel">
          <div class="item">
            <img
              src="./media/bimanual_actions_real/690_Fold the towel in half, aligning the left and right sides..png"
              class="image"
            />
            <div class="overlay">
              Fold the towel in half, aligning the left and right sides.
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_real/114_Fold the tshirt in half, with the top side overlapping the bottom..png"
              class="image"
            />
            <div class="overlay">
              Fold the tshirt in half, with the top side overlapping the bottom.
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_real/157_Fold the waistband of the dress in half, from left to right..png"
              class="image"
            />
            <div class="overlay">
              Fold the waistband of the dress in half, from left to right.
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_real/899_Fold the trousers, making a crease from the top to the bottom..png"
              class="image"
            />
            <div class="overlay">
              Fold the trousers, making a crease from the top to the bottom.
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_real/130_Fold the tshirt in half, aligning the left and right sides..png"
              class="image"
            />
            <div class="overlay">
              Fold the tshirt in half, aligning the left and right sides.
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_real/863_Fold the trousers, left side over right side..png"
              class="image"
            />
            <div class="overlay">
              Fold the trousers, left side over right side.
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_real/85_Fold the tshirt, making sure the top side touches the bottom side..png"
              class="image"
            />
            <div class="overlay">
              Fold the tshirt, making sure the top side touches the bottom side.
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_real/2_Fold the left sleeve to the center..png"
              class="image"
            />
            <div class="overlay">Fold the left sleeve to the center.</div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_real/236_Bring the top side of the skirt towards the bottom side and fold them in half..png"
              class="image"
            />
            <div class="overlay">
              Bring the top side of the skirt towards the bottom side and fold
              them in half.
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_real/22_Fold the right sleeve towards the body..png"
              class="image"
            />
            <div class="overlay">Fold the right sleeve towards the body.</div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_real/64_Bend the tshirt in half, from left to right..png"
              class="image"
            />
            <div class="overlay">
              Bend the tshirt in half, from left to right.
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_real/834_Fold the trousers in half, with the left side overlapping the right..png"
              class="image"
            />
            <div class="overlay">
              Fold the trousers in half, with the left side overlapping the
              right.
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_real/3_Fold the tshirt, top side over bottom side..png"
              class="image"
            />
            <div class="overlay">
              Fold the tshirt, top side over bottom side.
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_real/415_Create a fold in the towel, going from top to bottom..png"
              class="image"
            />
            <div class="overlay">
              Create a fold in the towel, going from top to bottom.
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_real/843_Fold the trousers, top side over bottom side..png"
              class="image"
            />
            <div class="overlay">
              Fold the trousers, top side over bottom side.
            </div>
          </div>
          <div class="item">
            <img
              src="./media/bimanual_actions_real/570_Fold the cloth in half, aligning the top and bottom sides..png"
              class="image"
            />
            <div class="overlay">
              Fold the cloth in half, aligning the top and bottom sides.
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 id="dataset_generation" class="title is-3">
              Dataset generation
            </h2>
            <div class="content has-text-justified">
              <p>
                The
                <a href="https://huggingface.co/datasets/robotflow/vr-folding"
                  >VR-Folding dataset</a
                >
                does not include annotations for the language-conditioned
                manipulation task we tackle nor action-level segmentation of the
                actions or natural language annotations. For each instance, if
                one of the hands of the demonstrator is grasping a point, the
                dataset provides the indices of the closest vertices of the
                simulation mesh. Therefore, there is no information on how the
                hand approaches the garment. Additionally, the only perceptual
                input is a point cloud of fixed size. No RGB or depth images are
                available. The process we propose does not require human
                intervention, contrary to the one used by
                <a href="https://sites.google.com/view/language-deformable"
                  >Deng <i>et al.</i></a
                >
                for collecting the unimanual dataset , and therefore can be
                easily scaled.
              </p>
            </div>
            <h3 id="rendering" class="title is-4">Rendering</h3>

            <div class="content has-text-justified">
              <p>
                The VR-Folding dataset does not contain RGB-D inputs, and the
                provided simulation meshes are un-textured. Moreover, the
                garments used in the simulator have a constant color for the
                interior and a repetitive texture with a differentiated color
                that yields colored point clouds:
              </p>
              <div class="columns is-vcentered">
                <div class="column is-one-quarter">
                  <img src="./media/dataset_sample/000045.jpg" />
                </div>
                <div class="column is-one-quarter">
                  <img src="./media/dataset_sample/000105.jpg" />
                </div>
                <div class="column is-one-quarter">
                  <img src="./media/dataset_sample/000155.jpg" />
                </div>
                <div class="column is-one-quarter">
                  <img src="./media/dataset_sample/000210.jpg" />
                </div>
              </div>
              <p>
                This design choice can lead to inputs that can be more easily
                registered. For example, one can identify the interior and
                exterior, self-intersections, and the scale of the cloth.
                However, this texturing could limit the generalization
                capabilities of our RGB-based models for the same reason,
                causing the learning process to focus only on such patterns and
                differentiating the few colors seen during training.
              </p>
              <p>
                While VR-Folding obtains the simulation meshes by manipulating
                assets extracted from
                <a href="https://hbertiche.github.io/CLOTH3D/">CLOTH3D</a>, they
                are re-meshed to obtain triangular faces from the original
                quadratic faces. For this reason, applying face textures cannot
                be done straightforwardly and requires a first step of texture
                baking. After assigning each vertex and triangular face to the
                original CLOTH3D assets, we can transfer the cloth texture to
                the simulation meshes. When assigning a material to the mesh, we
                use a material definition from
                <a href="https://sites.google.com/view/clothesnet/"
                  >ClothesNet</a
                >, which effectively achieves a realistic cloth effect:
              </p>
              <pre><code>Ns 28.763235
Ka 1.000000 1.000000 1.000000
Ks 0.075000 0.075000 0.075000
Ke 0.000000 0.000000 0.000000
Ni 1.450000
d 1.000000
illum 2</code></pre>
              <p>
                We render RGB-D images using
                <a href="https://github.com/DLR-RM/BlenderProc">BlenderProc2</a>
                with cameras pointing at the object whose position we randomly
                sample from the volume delimited by two spherical caps of
                different radii with centers on the manipulated object.
                Concretely, we define the volume using elevations [45&deg,
                90&deg] and radii [1.8, 2.2]. We use the same camera position
                for all the steps of the same sequence. Finally, we render RGB-D
                images with a resolution of 384x384 pixels. We include an
                example of the original input and our newly introduced samples
                below.
              </p>
              <figure>
                <img src="./media/render_draw.png" />
                <figcaption>
                  <b>Re-rendering step: </b>The only visual input in the
                  original dataset is given as a colored point cloud with
                  uniform colors and patterns (left). We take the simulation
                  mesh, and randomly choose camera position (center). Finally,
                  we apply a texture to the mesh and render RGB-D images
                  (right).
                </figcaption>
              </figure>
              <p>
                The decision to use random cameras rather than fixed cameras, as
                in
                <a href="https://sites.google.com/view/language-deformable"
                  >Deng <i>et al.</i></a
                >, is one of the reasons why our dataset is challenging. When
                using random camera positions, the input clothes have different
                sizes, and their shape is more affected by perspective as the
                camera deviates from the zenithal position. We ensure that all
                the pick and place positions fall inside the image and resample
                a new camera to generate the complete sequence again otherwise.
              </p>
            </div>

            <h3 id="language_annotations" class="title is-4">
              Language annotations
            </h3>

            <div class="content has-text-justified">
              <p>
                When annotating bimanual actions using
                <a href="https://arxiv.org/abs/1901.02970">NOCS coordinates</a>,
                it is usual that the left and right pickers use different
                semantic locations, e.g., the left picker grabs the top right
                part, and the right picker grabs the bottom right part. In this
                case, we can infer that the common objective is to hold the
                right part of the garment, but one cannot trivially resolve many
                other situations. To do that, we designed a heuristics detailed
                below that outputs a common semantic location considering the
                positions of the left and right pickers given the context of the
                action. $$ \begin{align} &\textbf{Inputs: }\ l_v, l_h,r_v, r_h,
                \texttt{type}, \texttt{garment}, s_{\text{pick}} \text{ (if
                }\texttt{type="place"}\text{)} \\ &\textbf{Output:}\
                \text{Semantic location. Optionally a sleeve flag} \\ & v
                \leftarrow l_v \text{ if } l_v=r_v \text{ else } \text{NULL}
                \quad \triangleleft\text{Same position in top-bottom plane} \\&
                h \leftarrow l_h \text{ if } l_h=r_h \text{ else } \text{NULL}
                \quad \triangleleft\text{Same position in left-right plane} \\ &
                \textbf{if } h \neq \text{NULL} \\& \quad \textbf{if } v \neq
                \text{NULL} \\ &\quad \quad \textbf{if }
                \texttt{type}=\texttt{"place"} \\ &\quad \quad \quad
                \triangleleft \text{Avoid same pick&place} \\ &\quad \quad \quad
                \textbf{if } s_{\text{pick}} = h \textbf{ return } v \\ &\quad
                \quad \quad \textbf{else-if } s_{\text{pick}} = v \textbf{
                return } h \\ &\quad \quad \quad \textbf{else-if }
                s_{\text{pick}}\ \text{opposite of}\ v \textbf{ return } h \\
                &\quad \quad \quad \textbf{else-if } s_{\text{pick}}\
                \text{opposite of}\ h \textbf{ return } v \\ &\quad \quad \quad
                \textbf{else} \textbf{ return } v + \texttt{" "}+h \\ &\quad
                \quad \textbf{else} \\ &\quad \quad \quad \textbf{if }
                \texttt{garment}=\texttt{"tshirt"} \text{ and }v\text{ is top
                }\textbf{ then} \\ & \quad\quad\quad\quad \textbf{return }h
                \text{ and sleeve flag.} \\ &\quad \quad \quad \textbf{else}
                \textbf{ return } v + \texttt{" "}+h \\ & \quad
                \textbf{else}\\&\quad\quad\textbf{ return }h\\ & \textbf{else}
                \\ & \quad \textbf{if } v \neq \text{NULL} \\ & \quad\quad
                \textbf{return }h \\ & \quad \textbf{else} \\ & \quad\quad
                \textbf{if }\texttt{type}=\texttt{"place"} \\ & \quad\quad\quad
                \textbf{return }\text{Opposite of }s_{\text{pick}} \\ &
                \textbf{Raise error if not returned} \\ \end{align} $$ where
                \(l_v, l_h, r_v, r_h\) are the semantic locations for the left
                (\(l\)) and right (\(r\)) picker along the top-bottom (\(v\))
                and left-right planes (\(h\)), and \(s_{\text{pick}}\) is the
                semantic pick location. \(\texttt{type}\) can be
                \(\texttt{"pick"}\) or \(\texttt{"place"}\) and
                \(\texttt{"garment"}\) is the garment type. Note that when the
                sleeve flag is returned, the place action is irrelevant. In some
                cases, observe that we use both semantic locations together.
              </p>
              <figure>
                <img src="./media/nocs.png" />
                <figcaption>
                  <b>Semantic pick and place positions: </b>
                  We obtain the semantic location of the grip by mapping the
                  picked vertices on the NOCS and thresholding its coordinates.
                  In this figure, we show an example of each category colored by
                  thresholding the left-right and top-bottom directions. We do
                  not threshold the front-rear direction as it is not relevant
                  for the considered actions.
                </figcaption>
              </figure>
              <p>
                Once the semantic location of the pick and place positions are
                known, we assign a language instruction to the folding
                sub-action. We make use of template prompts below and include
                the complete list below. We can distinguish three different
                kinds of actions: sleeve manipulations, refinements, and generic
                folds. For all of them, the prompts follow a template where the
                placeholders surrounded by brackets, i.e.,
                <code>{which}</code> and <code>{garment}</code>, are replaced by
                the pick and place semantic locations and the garment type,
                respectively. If the heuristics returns a sleeve flag, we use a
                prompt from the set of language instructions for sleeves with
                the semantic pick position. If the pick and place position is
                the same, we assume that the volunteer is performing a
                refinement and uses one of the prompts from a predefined list of
                language templates for small actions. Otherwise, we use a
                generic fold prompt from a set of generic instructions.
              </p>
              <div class="columns">
                <div class="column is-one-third">
                  <table>
                    <tr class="header" style="text-align: center">
                      <th>Sleeve manipulations</th>
                    </tr>
                    <tr>
                      <td>
                        Fold the
                        <code>{which}</code>
                        sleeve towards the inside.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Fold the
                        <code>{which}</code>
                        sleeve.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Fold the
                        <code>{which}</code>
                        sleeve towards the body.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Bend the
                        <code>{which}</code>
                        sleeve towards the inside.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Fold the
                        <code>{which}</code>
                        sleeve to the center.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Fold the
                        <code>{which}</code>
                        sleeve towards the middle.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Bring the
                        <code>{which}</code>
                        sleeve to the center.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Fold the
                        <code>{which}</code>
                        sleeve inward to the halfway point.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Tuck the
                        <code>{which}</code>
                        sleeve towards the center.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Meet the
                        <code>{which}</code>
                        sleeve at the center.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Fold the
                        <code>{which}</code>
                        sleeve to the midpoint.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Center the
                        <code>{which}</code>
                        sleeve.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Align the
                        <code>{which}</code>
                        sleeve to the center.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Fold the
                        <code>{which}</code>
                        sleeve to the axis.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Bring the
                        <code>{which}</code>
                        sleeve to the median.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Fold the
                        <code>{which}</code>
                        sleeve to the central point.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Fold the
                        <code>{which}</code>
                        sleeve towards the midpoint of the shirt.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Bring the
                        <code>{which}</code>
                        sleeve to the center seam.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Fold the
                        <code>{which}</code>
                        sleeve to the centerline of the shirt.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Fold the
                        <code>{which}</code>
                        sleeve to the centerline of the shirt.
                      </td>
                    </tr>
                  </table>
                </div>
                <div class="column is-one-third">
                  <table>
                    <tr class="header" style="text-align: center">
                      <th>Small refinements</th>
                    </tr>
                    <tr>
                      <td>
                        Fold the <code>{which}</code> part of the
                        <code>{garment}</code> neatly.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Align the <code>{which}</code> part of the
                        <code>{garment}</code> properly.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Arrange the <code>{which}</code> part of the
                        <code>{garment}</code> neatly.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Straighten out the <code>{which}</code> part of the
                        <code>{garment}</code>.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Place the <code>{which}</code> part of the
                        <code>{garment}</code> in the correct position.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Ensure the <code>{which}</code> part of the
                        <code>{garment}</code> is well-positioned.
                      </td>
                    </tr>
                  </table>
                </div>
                <div class="column is-one-third">
                  <table>
                    <tr class="header">
                      <th>Folds</th>
                    </tr>
                    <tr>
                      <td>
                        Fold the <code>{garment}</code> in half,
                        <code>{from}</code> to <code>{to}</code>.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Fold the <code>{garment}</code> from the
                        <code>{from}</code> side towards the
                        <code>{to}</code> side.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Fold the <code>{garment}</code> in half, starting from
                        the <code>{from}</code> and ending at the
                        <code>{to}</code>.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Fold the <code>{garment}</code>,
                        <code>{from}</code> side over <code>{to}</code> side.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Bend the <code>{garment}</code> in half, from
                        <code>{from}</code> to <code>{to}</code>.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Fold the <code>{garment}</code>, making sure the
                        <code>{from}</code> side touches the
                        <code>{to}</code> side.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Fold the <code>{garment}</code>, bringing the
                        <code>{from}</code> side to meet the
                        <code>{to}</code> side.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Crease the <code>{garment}</code> down the middle, from
                        <code>{from}</code> to <code>{to}</code>.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Fold the <code>{garment}</code> in half horizontally,
                        <code>{from}</code> to <code>{to}</code>.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Make a fold in the <code>{garment}</code>, starting from
                        the <code>{from}</code> and ending at the
                        <code>{to}</code>.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Fold the <code>{garment}</code> in half, aligning the
                        <code>{from}</code> and <code>{to}</code> sides.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Fold the <code>{garment}</code>, ensuring the
                        <code>{from}</code> side meets the
                        <code>{to}</code> side.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Fold the <code>{garment}</code>, orientating from the
                        <code>{from}</code> towards the <code>{to}</code>.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Fold the <code>{garment}</code> cleanly, from the
                        <code>{from}</code> side to the <code>{to}</code> side.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Fold the <code>{garment}</code> in half, with the
                        <code>{from}</code> side overlapping the
                        <code>{to}</code>.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Create a fold in the <code>{garment}</code>, going from
                        <code>{from}</code> to <code>{to}</code>.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Bring the <code>{from}</code> side of the
                        <code>{garment}</code> towards the
                        <code>{to}</code> side and fold them in half.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Fold the waistband of the <code>{garment}</code> in
                        half, from <code>{from}</code> to <code>{to}</code>.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Fold the <code>{garment}</code> neatly, from the
                        <code>{from}</code> side to the <code>{to}</code> side.
                      </td>
                    </tr>
                    <tr>
                      <td>
                        Fold the <code>{garment}</code>, making a crease from
                        the <code>{from}</code> to the <code>{to}</code>.
                      </td>
                    </tr>
                  </table>
                </div>
              </div>
            </div>

            <h3 id="filtering" class="title is-4">
              Filtering out divergent sequences
            </h3>

            <div class="content has-text-justified">
              <p>
                Simulating clothes is a very challenging task for which
                <a href="https://sites.google.com/view/cloth-sim2real-benchmark"
                  >some simulators exhibit a large gap with reality</a
                >. Additionally, the constrained optimization routines used by
                some simulators might lead to unstable solutions where the
                predicted cloth vertices diverge. We found several sequences of
                the VR-Folding dataset where the clothes underwent this
                phenomenon and present one of them below.
              </p>
              <figure>
                <div class="columns is-vcentered">
                  <div class="column is-one-quarter">
                    <img
                      src="./media/divergent_sequence/00156_Top_000013_000050.png"
                    />
                    <figcaption>t=50</figcaption>
                  </div>
                  <div class="column is-one-quarter">
                    <img
                      src="./media/divergent_sequence/00156_Top_000013_000055.png"
                    />
                    <figcaption>t=55</figcaption>
                  </div>
                  <div class="column is-one-quarter">
                    <img
                      src="./media/divergent_sequence/00156_Top_000013_000075.png"
                    />
                    <figcaption>t=75</figcaption>
                  </div>
                  <div class="column is-one-quarter">
                    <img
                      src="./media/divergent_sequence/00156_Top_000013_000195.png"
                    />
                    <figcaption>t=195</figcaption>
                  </div>
                </div>
                <figcaption>
                  <b>Divergent sequence: </b>In this sequence, we can see that
                  the manipulated top has become unstable in the cloth
                  simulator, yielding unrealistic shapes. The top uses the
                  CLOTH3D mesh with identifier 00156 and corresponds to the
                  sequence 00156_Top_000013_t in the VR-Folding dataset, where
                  the dataset uses time notation t in steps of 5, i.e., t=50 and
                  t=55 are consecutive frames.
                </figcaption>
              </figure>
              <p>
                In the above figure, we can observe that when the simulator
                becomes unstable, some of the vertex positions are excessively
                far, creating unusually long edges. To remove this and other
                occurrences of this phenomenon, we compute the set of edge
                lengths \begin{align}
                \mathcal{E}_{\text{lengths}}:=\{\left\lVert \mathbf{v}_i -
                \mathbf{v}_j\right\rVert: (\mathbf{v}_i,\mathbf{v}_j)\in
                \mathcal{E}\}\,, \end{align} where \(\mathcal{E}\) is the set of
                vertices of the mesh. Then, we compute the quantity
                \begin{align} \frac{\text{max}\left[
                \mathcal{E}_{\text{lengths}}\right] - \mathbb{E}\left[
                \mathcal{E}_{\text{lengths}}\right]}{\sqrt{\text{VAR}\left[\mathcal{E}_{\text{lengths}}\right]}}\,,
                \end{align} wich computes how much the maximum edge length
                deviates from the mean normalized by the standard deviation.
                Finally, we filter out all meshes for which the ratio of the
                quantity in the equation above between the mesh of interest and
                the NOCS mesh exceeds 3.5. Note that NOCS has a different scale
                but the ratio of standard deviations removes this effect.
              </p>
            </div>

            <h3 id="statistics" class="title is-4">Statistics</h3>

            <div class="content has-text-justified">
              <p>
                The VR-Folding dataset contains almost 4000 bimanual folding
                demonstrations from humans. The demonstrations are performed
                with a large variety of meshes, being almost all of them
                distinct. From these sequences, we are able to segment around
                7000 actions and obtain aligned text instructions. By means of
                our proposed pipeline, we are able to obtain a diverse set of
                language instructions, totaling more than 1000 unique prompts,
                as seen in the table below.
              </p>
              <table>
                <tr class="header">
                  <th>Garment</th>
                  <th># demos</th>
                  <th># meshes</th>
                  <th># actions</th>
                  <th># instructions</th>
                </tr>
                <tr>
                  <td>Skirt</td>
                  <td>464</td>
                  <td>462</td>
                  <td>857</td>
                  <td>196</td>
                </tr>
                <tr>
                  <td>Top</td>
                  <td>893</td>
                  <td>890</td>
                  <td>891</td>
                  <td>262</td>
                </tr>
                <tr>
                  <td>Trousers</td>
                  <td>1541</td>
                  <td>1539</td>
                  <td>2770</td>
                  <td>353</td>
                </tr>
                <tr>
                  <td>T-shirt</td>
                  <td>993</td>
                  <td>990</td>
                  <td>2444</td>
                  <td>254</td>
                </tr>
              </table>
              <p>
                In the figure below, we show a histogram of the number of
                actions of each sequence grouped by clothing category in linear
                scale and stacked (left) and logarithmic scale and separated
                (right) for the BiFold dataset. Note that some sequences can
                have only one action as the sequence can be truncated by some
                filtering step and the valid folding sub-action can still be
                useful for training. We can see that there are sequences with up
                to six parsed actions, showing the clear need for our novel
                pipeline for action parsing and annotation as the demonstrators
                do not follow the predefined instructions.
              </p>
              <div class="columns is-vcentered">
                <div class="column is-one-half">
                  <img
                    src="./media/dataset_statistics/number_of_actions_per_category-1.png"
                  />
                </div>
                <div class="column is-one-half">
                  <img
                    src="./media/dataset_statistics/log_number_of_actions_per_category-1.png"
                  />
                </div>
              </div>
              <p>
                Below, we show the distribution of the semantic locations of the
                origin and end of the manipulation actions. While in the up-down
                folds, most volunteers prefer to fold top to down, the folds
                along the left-right direction present an almost equal number of
                samples from left to right than from right to left.
              </p>
              <img
                style="
                  display: block;
                  width: 50%;
                  margin-left: auto;
                  margin-right: auto;
                "
                src="./media/dataset_statistics/which_folds-1.png"
              />
              <p>
                When referring to folding sleeves, the next figure shows a
                similar distribution of equal preference for the left and right
                arm with the latter being slightly more used for the first fold.
              </p>
              <img
                style="
                  display: block;
                  width: 50%;
                  margin-left: auto;
                  margin-right: auto;
                "
                src="./media/dataset_statistics/which_sleeve_folded_first-1.png"
              />
              <p>
                Finally, we can see that there is a non-negligible number of
                refinement actions, which the volunteers use to correct
                suboptimal folds. The existence of the suboptimal folds is the
                main motivation for the context that BiFold incorporates, which
                allows us to keep track of the previous actions.
              </p>
              <img
                style="
                  display: block;
                  width: 50%;
                  margin-left: auto;
                  margin-right: auto;
                "
                src="./media/dataset_statistics/number_of_refinement_actions-1.png"
              />
            </div>
            <h2 id="experimental_setup" class="title is-3">
              Experimental setup
            </h2>

            <h3 id="modeling" class="title is-4">Modeling</h3>
            <div class="content has-text-justified">
              <p>
                For each dataset, we train a single model for all the cloth
                categories using the resolution of the images in the training
                set. That means unimanual models ingest a square image with a
                resolution of 224 pixels, while bimanual models use a higher
                resolution of 384. All models use a
                <a href="https://arxiv.org/abs/2010.11929"
                  >Vision Transformer image backbone</a
                >
                that tokenizes the input image by taking 16x16 patches. We use
                the
                <a
                  href="https://huggingface.co/docs/transformers/en/model_doc/siglip"
                  >SigLIP base model in Hugginface</a
                >. We process the RGB images and the input text using the
                default SigLIP processor. During inference of the bimanual
                models, we apply the mask to the input image and fill the rest
                with the background color of the training images.
              </p>
              <p>
                The SigLIP model is trained with a contrastive objective similar
                to <a href="https://arxiv.org/abs/2103.00020">CLIP</a> and hence
                learns to extract a single image embedding and a text embedding
                that lie in the same space and are close if they have semantic
                similarities. Instead, we are interested in retrieving tokens
                for the image and language information to be fused using a
                <a href="https://arxiv.org/abs/1706.03762">transformer</a>.
                Doing so allows incorporating additional conditioning signals as
                we do in the BiFold version with context and has the potential
                of adding new modalities. Moreover, this formulation allows the
                processing of the tokens corresponding to the input image and
                transforming them back to the image domain to obtain value maps.
                The tokens we use are the last hidden states of SigLIP, which
                have dimension 768.
              </p>
              <p>
                The pretraining of SigLIP on large-scale datasets enables us to
                learn the invariances of images from data and create a
                high-level representation of images and text. However, the
                pretraining objective may focus on distinctive parts of the text
                that can make a discriminative embedding on the shared space,
                which does not necessarily align with the representations needed
                for our problem. With this in mind, we use
                <a href="https://arxiv.org/abs/2106.09685">LoRA</a>, allowing us
                to modify the inner activations while retaining the knowledge
                that would not be possible to learn from small datasets such as
                that from
                <a href="https://sites.google.com/view/language-deformable"
                  >Deng <i>et al.</i></a
                >.
              </p>
              <p>
                Once we obtain the output tokens of the SigLIP model with the
                LoRA modifications, we append a class token to indicate the
                modality of each input and concatenate the result, i.e., one for
                RGB images and one for text. For the version with context, we
                process each RGB image separately, add a single RGB image class
                token, and add a learned positional embedding element-wise to be
                able to distinguish time steps and pixel positions. The
                resulting tokens are processed using a transformer encoder with
                8 blocks having 16 heads each and dimensions 4 times that of the
                input. The convolutional decoder heads all have the same
                architecture, which consists of 5 2D convolutional layers with
                kernel size 1 that halve the number of channels every two layers
                until reaching a single channel. Each of the layers but the last
                one is followed by upsampling layers with a scale factor of 2,
                yielding an image with the same resolution as the input.
              </p>
              <p>
                Besides the ablations provided in the paper, we experimented
                with other design choices that proved unsuccessful during the
                early development of this project:
              </p>
              <ul>
                <li>
                  <strong>Decoder models: </strong>Instead of relying on
                  convolutional decoders to obtain the heatmaps, we tried
                  replacing them with transformer decoders similar to those in
                  <a href="https://arxiv.org/abs/2111.06377">MAE</a>.
                  Nevertheless, the performance decreased in this case, and the
                  predicted heatmaps had patch artifacts. We show an example of
                  such artifacts below.
                </li>
                <br />
                <img
                  style="
                    display: block;
                    width: 50%;
                    margin-left: auto;
                    margin-right: auto;
                  "
                  src="./media/49.png"
                />
                <br />
                <li>
                  <strong>Predicting segmentation masks:</strong> Since the pick
                  positions have to fall into the cloth region, we use a
                  segmentation mask to enforce this. Instead of assuming that
                  the mask is known or relying on an out-of-the-box segmentation
                  model as we do for real-world samples, we experimented with
                  how to guide a constrained pick prediction without the cloth
                  region as input. To do that, we added an extra decoder head
                  and supervised its output with either cross-entropy loss or a
                  combination of dice and focal losses as done to train
                  state-of-the-art segmentation models like SAME. The output of
                  this model was then used to restrict the domain of the
                  predicted pick heatmaps. Despite promising and useful in
                  waiving the requirement for input masks, this method offered
                  worse performance.
                </li>
                <li>
                  <strong
                    >Conditioning place position on the pick position: </strong
                  >Given the interplay between the prediction of pick and place
                  positions, we experimented with an ancestral sampling
                  approach. To do that, we first predicted pick positions and
                  then used the output to condition the place prediction. This
                  method offered no notable benefits, showing that the tokens at
                  the output of the decoder contain enough information that the
                  decoders know how to multiplex to obtain the correct pick and
                  place positions.
                </li>
              </ul>
            </div>
            <h3 id="simulation" class="title is-4">Simulation</h3>
            <div class="content has-text-justified">
              <p>
                The pick and place manipulation primitive uses the following
                steps:
              </p>
              <ol>
                <li>
                  Set pick and place heights using the radius of the picker,
                  regardless of the world coordinate of the vertex.
                </li>
                <li>
                  The picker is moved to the picking position but at a
                  predefined height.
                </li>
                <li>
                  The picker moves to the pick position and closes the gripper.
                </li>
                <li>The picker moves to the position in 2.</li>
                <li>
                  The picker is moved to the placing position but at a
                  predefined height.
                </li>
                <li>
                  The picker goes to the placing position and opens the gripper.
                </li>
                <li>
                  The picker moves to the place position at the same predefined
                  height as in 2.
                </li>
              </ol>
              <p>
                All the movements are performed at a speed of 5 mm/action except
                steps 2 and 7, which we perform 100 times faster as they are
                supposed to not interact with the cloth. The bimanual primitive
                uses the unimanual primitive with the actions executed at the
                same time for both pickers.
              </p>
            </div>
            <h3 id="real" class="title is-4">Real</h3>
            <div class="content has-text-justified">
              <p>
                To obtain segmentation masks, we use the
                <a href="https://segment-anything.com/"
                  >ViT-h model variant of SAM</a
                >
                using pixel coordinates as input prompts. The masks are then
                used to determine square crops of the images provided by the
                Azure Kinect camera, which provides 1280x720 pixel images. To
                reduce the influence of the noise of the depth sensor, we take
                10 images and use the median value for each pixel. In
                particular, we compose axis aligned bounding boxes taking all
                the images of a given folding sequence. Then, we crop the images
                to a size determined as the maximum side length of the bounding
                box of all the images and a margin of 10 pixels added to prevent
                pick and place positions from falling on the border of the
                image. If needed, we pad the images using constant padding with
                value 0. We present the resulting cropped images for the clothes
                used in the real setup below.
              </p>
              <p>
                We evaluate our model on eight pieces of cloth: The checkered
                rag and small towel from the
                <a
                  href="https://www.iri.upc.edu/groups/perception/ClothObjectSet/"
                  >public household dataset</a
                >, two jeans, a long-sleeved and two short-sleeved T-shirts, and
                a dress. The rag, towel, and dress are unseen clothing
                categories. The jeans have a new material different from the
                smooth fabrics obtained in rendering. The long-sleeved T-shirt
                is a double-layer tee that does not appear in any training
                example. Besides, we record the dataset in a real setup with new
                lighting conditions, obtain shadows and reflexes, and the
                garments have different textures. Overall, the real dataset has
                a significant shift in the distribution of the inputs. Below, we
                show images of the eight garments of the real dataset in the
                initial configuration, which have different materials,
                topologies and textures.
              </p>
              <div class="columns is-multiline">
                <div class="column is-one-quarter-desktop is-half-tablet">
                  <div class="card">
                    <div class="card-image">
                      <img src="./media/real_dataset/0000_towel_0_0000.png" />
                    </div>
                    <footer class="card-footer">
                      <a
                        class="card-footer-item"
                        href="https://www.iri.upc.edu/groups/perception/ClothObjectSet/"
                        >Checkered rag
                      </a>
                    </footer>
                  </div>
                </div>
                <div class="column is-one-quarter-desktop is-half-tablet">
                  <div class="card">
                    <div class="card-image">
                      <img src="./media/real_dataset/0001_towel_0_0000.png" />
                    </div>
                    <footer class="card-footer">
                      <a
                        class="card-footer-item"
                        href="https://www.iri.upc.edu/groups/perception/ClothObjectSet/"
                        >Small towel
                      </a>
                    </footer>
                  </div>
                </div>
                <div class="column is-one-quarter-desktop is-half-tablet">
                  <div class="card">
                    <div class="card-image">
                      <img src="./media/real_dataset/0002_pants_0_0000.png" />
                    </div>
                    <footer class="card-footer">
                      <div class="card-footer-item">Jeans 1</div>
                    </footer>
                  </div>
                </div>
                <div class="column is-one-quarter-desktop is-half-tablet">
                  <div class="card">
                    <div class="card-image">
                      <img src="./media/real_dataset/0003_pants_0_0000.png" />
                    </div>
                    <footer class="card-footer">
                      <div class="card-footer-item">Jeans 2</div>
                    </footer>
                  </div>
                </div>
                <div class="column is-one-quarter-desktop is-half-tablet">
                  <div class="card">
                    <div class="card-image">
                      <img
                        src="./media/real_dataset/0004_long_shirt_0_0000.png"
                      />
                    </div>
                    <footer class="card-footer">
                      <div class="card-footer-item">Long-sleeved T-shirt</div>
                    </footer>
                  </div>
                </div>
                <div class="column is-one-quarter-desktop is-half-tablet">
                  <div class="card">
                    <div class="card-image">
                      <img
                        src="./media/real_dataset/0005_short_shirt_0_0000.png"
                      />
                    </div>
                    <footer class="card-footer">
                      <div class="card-footer-item">
                        Short-sleeved T-shirt 1
                      </div>
                    </footer>
                  </div>
                </div>
                <div class="column is-one-quarter-desktop is-half-tablet">
                  <div class="card">
                    <div class="card-image">
                      <img
                        src="./media/real_dataset/0006_short_shirt_0_0000.png"
                      />
                    </div>
                    <footer class="card-footer">
                      <div class="card-footer-item">
                        Short-sleeved T-shirt 2
                      </div>
                    </footer>
                  </div>
                </div>
                <div class="column is-one-quarter-desktop is-half-tablet">
                  <div class="card">
                    <div class="card-image">
                      <img src="./media/real_dataset/0007_dress_0_0000.png" />
                    </div>
                    <footer class="card-footer">
                      <div class="card-footer-item">Dress</div>
                    </footer>
                  </div>
                </div>
              </div>
            </div>

            <h2 id="additional_experiments" class="title is-3">
              Additional experiments
            </h2>
            <h3 id="qualitative" class="title is-4">Qualitative evaluation</h3>
            <div class="content has-text-justified">
              <p>
                BiFold predicts a single pick-and-place action at a time. Hence,
                we do not expect it to perform a whole rollout with a single
                instruction (when performing an end-to-end folding, we provide a
                sequence of instructions). However, we found it interesting to
                probe the model with some out-of-distribution instructions. When
                prompted with <code>"Fold a T-shirt into a square"</code>, the
                model predicts an action that folds a part of the shirt inwards
                to make it more similar to a square. While the language
                instruction is new, we can expect this behavior as the dataset
                contains similar folding steps.
              </p>
              <div class="columns is-vcentered">
                <div class="column is-one-quarter">
                  <img src="./media/custom_prompts/1981_F.png" />
                </div>
                <div class="column is-one-quarter">
                  <img src="./media/custom_prompts/1982_F.png" />
                </div>
                <div class="column is-one-quarter">
                  <img src="./media/custom_prompts/1986_F.png" />
                </div>
                <div class="column is-one-quarter">
                  <img src="./media/custom_prompts/2006_F.png" />
                </div>
              </div>
              <div class="columns is-vcentered">
                <div class="column is-one-quarter">
                  <img src="./media/custom_prompts/2004_F.png" />
                </div>
                <div class="column is-one-quarter">
                  <img src="./media/custom_prompts/2188_F.png" />
                </div>
                <div class="column is-one-quarter">
                  <img src="./media/custom_prompts/2189_F.png" />
                </div>
                <div class="column is-one-quarter">
                  <img src="./media/custom_prompts/2213_F.png" />
                </div>
              </div>
              <p>
                When using the instruction
                <code>Fold the trousers in L shape</code> we obtain outputs like
                the ones presented below. In the first row, we see that BiFold
                ignores the instruction and performs the fold for the current
                image observation that is statistically more common in the
                dataset. In the second row, we can see that it tries different
                actions with a single hand that can. Notably, there were many
                more actions using only one hand for this instruction than on
                average, but the model does not replicate a way to achieve an
                L-fold that would grasp the end of one of the legs with one or
                two arms and move it diagonally up and to the other side, which
                we can expect due to the absence of similar folds in the
                dataset.
              </p>
              <div class="columns is-vcentered">
                <div class="column is-one-quarter">
                  <img src="./media/custom_prompts/4526_F.png" />
                </div>
                <div class="column is-one-quarter">
                  <img src="./media/custom_prompts/4533_F.png" />
                </div>
                <div class="column is-one-quarter">
                  <img src="./media/custom_prompts/4555_F.png" />
                </div>
                <div class="column is-one-quarter">
                  <img src="./media/custom_prompts/4613_F.png" />
                </div>
              </div>
              <div class="columns is-vcentered">
                <div class="column is-one-quarter">
                  <img src="./media/custom_prompts/4609_F.png" />
                </div>
                <div class="column is-one-quarter">
                  <img src="./media/custom_prompts/4556_F.png" />
                </div>
                <div class="column is-one-quarter">
                  <img src="./media/custom_prompts/4505_F.png" />
                </div>
                <div class="column is-one-quarter">
                  <img src="./media/custom_prompts/4602_F.png" />
                </div>
              </div>
            </div>
            <div class="content has-text-justified">
              <p>
                BiFold also experiments some failures, and below we present a
                collection of the worse bimanual action predictions obtained for
                each environment and cloth category:
              </p>
              <figure>
                <div class="columns">
                  <div class="column is-one-quarter">
                    <img
                      src="./media/failures/654_Fold the skirt, making sure the bottom side touches the top side..png"
                    />
                    <figcaption>
                      Fold the skirt, making sure the bottom side touches the
                      top side
                    </figcaption>
                  </div>
                  <div class="column is-one-quarter">
                    <img
                      src="./media/failures/652_Fold the skirt, making a crease from the left to the right..png"
                    />
                    <figcaption>
                      Fold the skirt, making a crease from the left to the right
                    </figcaption>
                  </div>
                  <div class="column is-one-quarter">
                    <img
                      src="./media/failures/298_Fold the top, right side over left side..png"
                    />
                    <figcaption>
                      Fold the top, right side over left side
                    </figcaption>
                  </div>
                  <div class="column is-one-quarter">
                    <img
                      src="./media/failures/39_Fold the skirt, top side over bottom side..png"
                    />
                    <figcaption>
                      Fold the skirt, top side over bottom side
                    </figcaption>
                  </div>
                </div>
              </figure>
              <figure>
                <div class="columns">
                  <div class="column is-one-quarter">
                    <img
                      src="./media/failures/01165_Tshirt_000151_000035_Center the right sleeve._True.png"
                    />
                    <figcaption>Center the right sleeve</figcaption>
                  </div>
                  <div class="column is-one-quarter">
                    <img
                      src="./media/failures/01444_Skirt_000113_000095_Create a fold in the skirt, going from top to bottom._False.png"
                    />
                    <figcaption>
                      Create a fold in the skirt, going from top to bottom
                    </figcaption>
                  </div>
                  <div class="column is-one-quarter">
                    <img
                      src="./media/failures/01505_Top_000190_000060_Fold the top neatly, from the bottom side to the top side._False.png"
                    />
                    <figcaption>
                      Fold the top neatly, from the bottom side to the top side
                    </figcaption>
                  </div>
                  <div class="column is-one-quarter">
                    <img
                      src="./media/failures/00331_Trousers_000069_000345_Crease the trousers down the middle, from top to bottom._False.png"
                    />
                    <figcaption>
                      Fold the skirt, top side over bottom side
                    </figcaption>
                  </div>
                </div>
              </figure>
              <figure>
                <div class="columns">
                  <div class="column is-one-quarter">
                    <img
                      src="./media/failures/120_Fold the tshirt in half, left to right..png"
                    />
                    <figcaption>
                      Fold the tshirt in half, left to right
                    </figcaption>
                  </div>
                  <div class="column is-one-quarter">
                    <img
                      src="./media/failures/564_Bend the cloth in half, from top to bottom..png"
                    />
                    <figcaption>
                      Bend the cloth in half, from top to bottom
                    </figcaption>
                  </div>
                  <div class="column is-one-quarter">
                    <img
                      src="./media/failures/278_Fold the towel neatly, from the left side to the right side..png"
                    />
                    <figcaption>
                      Fold the towel neatly, from the left side to the right
                      side
                    </figcaption>
                  </div>
                  <div class="column is-one-quarter">
                    <img
                      src="./media/failures/858_Fold the trousers neatly, from the top side to the bottom side..png"
                    />
                    <figcaption>
                      Fold the trousers neatly, from the top side to the bottom
                      side
                    </figcaption>
                  </div>
                </div>
              </figure>
            </div>
            <h3 id="quantitative" class="title is-4">
              Quantitative evaluation
            </h3>
            <div class="content has-text-justified">
              <p>
                In order to compute quantitative evaluations on the real-world
                images, we start by collecting human annotations. To do that, we
                set up a pipeline in which volunteers can indicate a bimanual
                folding pick-and-place action by indicating the origin and end
                of the movement of the left and right arms based on an image and
                a natural language instruction. We filter out invalid
                annotations and only keep the actions in which the pick
                positions fall inside the cloth region. Below we present some
                annotation examples, which may have one or several valid
                actions.
              </p>
              <div class="columns is-vcentered">
                <div class="column is-one-quarter">
                  <img src="./media/annotations/dress.png" />
                </div>
                <div class="column is-one-quarter">
                  <img src="./media/annotations/longshirt.png" />
                </div>
                <div class="column is-one-quarter">
                  <img src="./media/annotations/pants.png" />
                </div>
                <div class="column is-one-quarter">
                  <img src="./media/annotations/towel.png" />
                </div>
              </div>
              <p>
                We compute image-based performance metrics on the real-world
                images using the previous annotations. To augment the number of
                samples in the evaluation, we apply all the language templates
                of Tables 1 and 3 of the supplementary in the initial submission
                for sleeve manipulations and folds, respectively. By doing so,
                we achieve a 20x factor in the sample size and evaluate how the
                model behaves to changes in the language. The next table shows
                the quantitative results for BiFold with and without context and
                <a href="https://sites.google.com/view/language-deformable"
                  >Deng <i>et al.</i></a
                >.
              </p>
              <table>
                <tr class="header">
                  <th></th>
                  <th>AP<sub>5</sub> (&uarr;)</th>
                  <th>AP<sub>10</sub> (&uarr;)</th>
                  <th>AP<sub>20</sub> (&uarr;)</th>
                  <th>AP<sub>50</sub> (&uarr;)</th>
                  <th>KP-MSE (&darr;)</th>
                  <th>Quantile (%) (&uarr;)</th>
                </tr>
                <tr>
                  <td>
                    <a href="https://sites.google.com/view/language-deformable"
                      >Deng <i>et al.</i></a
                    >
                  </td>
                  <td>1.4</td>
                  <td>5.8</td>
                  <td>14.3</td>
                  <td>35.3</td>
                  <td>37.1</td>
                  <td>66.1</td>
                </tr>
                <tr>
                  <td>BiFold w/o context</td>
                  <td>15.8</td>
                  <td>35.6</td>
                  <td>56.6</td>
                  <td>81.0</td>
                  <td>15.5</td>
                  <td>93.1</td>
                </tr>
                <tr>
                  <td>BiFold</td>
                  <td>9.5</td>
                  <td>31.4</td>
                  <td>49.9</td>
                  <td>73.8</td>
                  <td>18.8</td>
                  <td>90.4</td>
                </tr>
              </table>
              <p>
                The added complexity and domain shift due to visual differences,
                new language prompts, and unseen garments show an overall
                decrease in performance from the image metrics on the
                simulation-based bimanual dataset. However, we can see that
                BiFold consistently outperforms the baseline, which suffers from
                the imperfect depth maps of the real world and the pseudo-ground
                truth segmentation masks as it has only seen perfect
                ground-truth masks and depths during training. We can see this
                in the low accuracy scores, i.e., average precision and keypoint
                distance, and in the quantile performance that indicates a
                radically less confident prediction that becomes closer to
                random. Given the significant sim-to-real gap, BiFold cannot
                successfully incorporate context this time, the plain version
                being better in real-world evaluation. We hypothesize that the
                reason is that the image representation is worse due to the
                reality gap, and the error accumulates in the processing of the
                current image and those in the context.
              </p>
            </div>

            <h2 id="limitations" class="title is-3">
              Limitations and future work
            </h2>

            <h3 id="related_work_limitations" class="title is-4">
              Previous approaches
            </h3>
            <div class="content has-text-justified">
              <p>
                Most of the previous approaches work with depth maps. While this
                design decision makes the model naturally invariant to texture
                and luminance changes, it also discards the color information,
                which is included in most sensing devices and provides useful
                cues to understand the cloth state. While the invariance to
                appearance changes theoretically reduces the sim-to-real gap,
                real depth sensors are noisy and yield imperfect point clouds.
                The majority of prior works only train on perfect depth maps and
                segmentation masks, which makes the reality gap larger in
                practice. Our solution is based on incorporating color
                information through a foundational model fine-tuned with LoRA to
                maintain the generalization capabilities gained with large-scale
                pretraining.
              </p>
              <br />
              <p>
                While the approach to predict pick positions on segmented point
                clouds that some prior works adopt has some benefits, it also
                has many drawbacks that motivated us to operate on the pixel
                space. The benefit is that, instead of predicting pixel
                coordinates, these methods predict a probability distribution
                over the points and selects the best of them for the pick
                position. Naturally, the point is in the 3D world where the
                robot operates and does not need to be back-projected, which
                could ease computing relations between points as Euclidean
                geometry in 3D is easier than the projective geometry that
                images have. One drawback of these class of methods is that, in
                practice, the point cloud needs to be downsampled and some
                methods can only work with a reduced number of vertices. As an
                example,
                <a href="https://sites.google.com/view/language-deformable"
                  >Deng <i>et al.</i></a
                >
                only keep 200 points. As shown in the next figure, for
                real-world point clouds this implies a huge reduction of the
                input size that hinders learning useful quantities.
              </p>
              <br />
              <div class="columns is-vcentered">
                <div class="column is-one-half">
                  <img src="./media/related_work_limitations/graph01.png" />
                </div>
                <div class="column is-one-half">
                  <img
                    src="./media/related_work_limitations/pointcloud_and_graph00.png"
                  />
                </div>
              </div>
              <p>
                In particular, the subsampling fails to cover relevant parts of
                the manipulated garment such as the edges and corners, where
                there is less density of points than in the center of the cloth.
                This limitation is also shared to some extent by other methods
                relying on processing downsampled point clouds. For example,
                <a href="https://unifolding.robotflow.ai/">UniFolding</a>
                processes an input point cloud by first downsampling it to 8,000
                points, quantizing their coordinates, and finally selecting
                4,000 of such quantized points (we get the number of sampled
                points from the
                <a
                  href="https://github.com/xiaoxiaoxh/UniFolding/blob/main/config/virtual_experiment_stage1/train_virtual_base.yaml\#L8"
                  >configuration</a
                >
                and the sampling strategy in the
                <a
                  href="https://github.com/xiaoxiaoxh/UniFolding/blob/fa481bc235b67a9cec2a16811ce4ed1f598c7c13/learning/datasets/vr_dataset.py\#L234"
                  >data processing part</a
                >
                of the official implementation). The sampling strategy is to
                randomly choose from the input set of points without
                replacement. This strategy may lead to poor coverage of the
                input point cloud as it is possible that the points of a given
                zone are not selected. This could be improved by performing FPS
                instead.
              </p>
              <br />
              <p>
                Even if using smarter sampling strategies such as FPS, the
                average number of pixels inside the cloth region, i.e., the
                effective number of possible pick positions, is on average
                17,171. We present the histogram of values below.
              </p>
              <br />
              <img
                style="
                  display: block;
                  width: 50%;
                  margin-left: auto;
                  margin-right: auto;
                "
                src="./media/related_work_limitations/mask_points.png"
              />
              <br />
              <p>
                We can see that, while the minimum number of pixels in the cloth
                region is 3,760, which is slightly under the number of points of
                <a href="https://unifolding.robotflow.ai/">UniFolding</a>, the
                majority of samples have a larger value, being 69,221 points the
                maximum. If considering the number of total pixels in the image,
                there are 147,456 possible place points.
              </p>
              <p>
                Some of the previous approaches predicting both the picking and
                placing position using a segmented point cloud of the
                manipulated garment. While all pick positions fall in the cloth
                region, as they must specify a part of the cloth for the grasp,
                this is not satisfied for the place positions of the bimanual
                cloth manipulation dataset. To put numbers to it, we compute the
                average distance in pixels between the ground truth place
                positions of a given arm (recall that our dataset provides eight
                contact points) and the segmentation mask of the cloth. Then, we
                take the maximum value between the previous distances for the
                right and left arm, yielding the maximum distance to a mask.
                Doing so, we obtain the following histogram, which shows that
                roughly 80% of the samples of the dataset have a place position
                with a non-zero distance to the segmentation mask. The large
                number of place positions out of the cloth region shows the
                importance of using the pixel space and not the cloth point
                cloud.
              </p>
              <br />
              <img
                style="
                  display: block;
                  width: 50%;
                  margin-left: auto;
                  margin-right: auto;
                "
                src="./media/related_work_limitations/out_of_face.png"
              />
              <br />
              <p>We include samples with large distances below.</p>
              <figure>
                <div class="columns is-vcentered">
                  <div class="column is-one-quarter">
                    <img
                      src="./media/related_work_limitations/large_distance_mask/00333_Skirt_000027_000175.png"
                    />
                    <figcaption>Distance = 105.1 px</figcaption>
                  </div>
                  <div class="column is-one-quarter">
                    <img
                      src="./media/related_work_limitations/large_distance_mask/01172_Skirt_000094_000225.png"
                    />
                    <figcaption>Distance = 86.1 px</figcaption>
                  </div>
                  <div class="column is-one-quarter">
                    <img
                      src="./media/related_work_limitations/large_distance_mask/07191_Skirt_000438_000165.png"
                    />
                    <figcaption>Distance = 78.2 px</figcaption>
                  </div>
                  <div class="column is-one-quarter">
                    <img
                      src="./media/related_work_limitations/large_distance_mask/02752_Skirt_000183_000130.png"
                    />
                    <figcaption>Distance = 78.2 px</figcaption>
                  </div>
                </div>
                <figcaption>
                  <b
                    >Extreme examples of place positions out of segmentation
                    mask: </b
                  >Pick and place positions for
                  <strong><font color="#00FF00">right</font></strong>
                  and
                  <strong><font color="#0000FF">left</font></strong>
                  actions are represented as the origin and endpoints of an
                  arrow. Each action uses eight vertices, which might be
                  distinct and fall into different pixels.
                </figcaption>
              </figure>
              <p>
                Another limitation more specific of
                <a href="https://sites.google.com/view/language-deformable"
                  >Deng <i>et al.</i></a
                >
                is that the processing of point clouds uses fixed thresholds.
                For example, the subsampling is performed after a voxelization
                step with a fixed grid size of 0.0125 m. Then, the authors
                create a graph from the resulting point cloud that serves as
                input to the
                <a href="https://sites.google.com/view/vcd-cloth"
                  >visual connectivity graph model</a
                >. To create the graph, the authors find vertex neighbors using
                an \(\ell_2\) ball with a radius of 0.045 m. Overall, this makes
                the approach highly dependent on the scale, meaning that scaling
                up the same garment would result in different input graphs. As a
                reference, we include an illustration with the point clouds of
                T-shirts from the real-world evaluation and the unimanual
                dataset below.
              </p>
              <figure>
                <div class="columns is-vcentered">
                  <div class="column is-one-half">
                    <img src="./media/related_work_limitations/side01.png" />
                    <figcaption>
                      Side view with separated point clouds.
                    </figcaption>
                  </div>
                  <div class="column is-one-half">
                    <img src="./media/related_work_limitations/top03.png" />
                    <figcaption>
                      Top view with superimposed point clouds.
                    </figcaption>
                  </div>
                </div>
                <figcaption>
                  <strong
                    >Input difference between real garments and those in the
                    unimanual dataset: </strong
                  >We present a comparison between two T-shirts from the
                  real-world dataset (in red) and the unimanual dataset (in pink
                  and yellow). As we can see, the captured real-world point
                  clouds are denser than those in the unimanual dataset
                  (~307,000 vs. ~5,000 points). Moreover, we can see a clear
                  difference in scale, with the simulated garment having a rough
                  length from top to bottom of only 30 cm.
                </figcaption>
              </figure>
              <p>
                Finally, another limitation common in the previous methods is
                that they cannot keep a running memory and take previous
                observations into account. By doing that, BiFold can resolve
                ambiguities using past observations (e.g., parts of the cloth
                that were visible but are not currently visible or symmetries
                that appear with some cloth states such as a garment folded in a
                square shape and perceptually similar up-down and left-right
                directions) and apply refinement actions to correct suboptimal
                actions.
              </p>
            </div>
            <!-- <h3 id="related_work_modeling" class="title is-4">
              Modeling differences
            </h3>
            <div class="content has-text-justified">
              <p>
                In this section, we compare our model side-to-side with that of
                <a href="https://sites.google.com/view/language-deformable"
                  >Deng <i>et al.</i></a
                >
                as it is the strongest baseline and the most similar work in the
                literature despite being focused on unimanual folding. While the
                architectures share some similarities, we propose changes
                significantly impacting the evaluated performance, especially
                for the bimanual task. While both works share some design
                choices, they also have significant differences. We summarize
                them below.
              </p>

              <ul>
                <li>
                  <strong>Similarities: </strong>
                  <ul>
                    <li>
                      <strong>Transformer encoder: </strong>In both cases, the
                      tokenized representations of the inputs are concatenated
                      and processed with a series of self-attention layers. This
                      is an increasingly popular architecture that has been
                      applied to innumerable problems in different domains. The
                      reason is that once inputs are tokenized, the problem can
                      be cast into a sequence-to-sequence prediction, and
                      transformers have proven incredibly effective at it.
                    </li>
                    <li>
                      <strong>Convolutional decoder: </strong>BiFold uses
                      convolutional decoders for the prediction of the pick and
                      place positions of each arm. Deng \etal
                      \cite{language_deformable_manipulation} instead only use
                      these for the place position prediction, while pick
                      positions are predicted as probability distributions over
                      the points of the down-sampled point cloud.
                    </li>
                  </ul>
                </li>
                <li>
                  <strong>Differences: </strong>
                  <ul>
                    <li>
                      <strong>Input: </strong>The input modalities that BiFold
                      can ingest are RGB images and text. On the contrary, Deng
                      \etal use depth images where a segmentation mask has been
                      applied. From these images, they also compute a point
                      cloud, which they downsample and use as input to a graph
                      embedding. To process depth maps, Deng \etal perform a
                      simple linear projection with weights trained from
                      scratch. Finally, they obtain language embeddings using
                      the text part of a CLIP model. Instead, BiFold takes RGB
                      images and text and uses a foundational model to process
                      both modalities and obtain aligned features. This
                      representation is fine-tuned with \gls{lora} layers to
                      include information relevant to the current task. Notably,
                      BiFold's input does not use a segmentation mask and can
                      exploit potentially advantageous color cues.
                    </li>
                    <li>
                      <strong>Incorporation of context: </strong>Perhaps the
                      most novel part of our architecture is the use of context.
                      By including information on previous manipulations, BiFold
                      can disambiguate some actions and better understand the
                      cloth state. This is especially useful after performing
                      unsuccessful actions and for identifying parts of the
                      garment that are occluded but were not in previous states.
                    </li>
                  </ul>
                </li>
              </ul>
            </div> -->

            <h3 id="dataset_extension" class="title is-4">BiFold dataset</h3>
            <div class="content has-text-justified">
              <p>
                Our dataset does not contain a variety of backgrounds, lighting,
                and materials. We address the lack of diversity of background by
                using image masking, and the large set of textures used in our
                dataset accounts for variability regarding the visual
                appearance. We obtain semantic locations based on \gls{nocs}
                thresholding, which is a reasonable approach but leverages the
                fact that the garments of the same category have a similar
                shape. Considering clothes with more variability such as those
                in the
                <a href="https://sites.google.com/view/clothesnet/"
                  >ClothesNet dataset</a
                >
                may require other approaches. One possibility is to use a
                contrastive vision-language model and find the nearest neighbor
                in the embedding space from a set of category-specific names,
                e.g., hem, collar, sleeve, cuff. This is similar to approaches
                that distill language features to 3D, a technique applied to
                robotics in
                <a href="https://f3rm.github.io/">Distilled Feature Fields</a>.
              </p>
            </div>

            <h3 id="modeling_and_evaluation" class="title is-4">
              BiFold model and evaluation
            </h3>
            <div class="content has-text-justified">
              <p>
                BiFold and all other approaches predicting value maps to infer
                pick and place positions are trained to minimize image-based
                metrics. As we can observe in the qualitative results that
                evaluate pixel accuracy in our bimanual dataset, the BiFold
                variant with context can obtain outstanding results. However, we
                are ultimately interested in improving the success metrics on
                folding actions instead. One of the problems is that using start
                and end positions close in the pixel space to the optimal
                positions may result in radically different configurations after
                performing the pick-and-place action. This is due to the nearly
                infinite degrees of freedom of clothes.
              </p>
              <p>
                When assessing the pick-and-place success, using a dataset of
                human demonstrations calls into question the evaluation
                procedure to compare to an oracle. As we have seen,
                demonstrations are sometimes bad, which can lead to confusing
                performance reports. While there exist
                <a href="https://ral-si.github.io/cloth-benchmark/"
                  >metrics to evaluate simple garment manipulation actions such
                  as folding in half </a
                >, some of the actions we include in our dataset, e.g.,
                refinement actions, cannot be directly evaluated with such
                metrics.
              </p>
              <p>
                Among the baselines considered in this work, Foldsformer and
                <a href="https://sites.google.com/view/language-deformable"
                  >Deng <i>et al.</i></a
                >
                use
                <a href="https://sites.google.com/view/softgym">SoftGym</a> as a
                simulator, while
                <a href="https://cliport.github.io/">CLIPort</a> relies on
                <a href="https://pybullet.org/wordpress/">PyBullet</a>. However,
                according to
                <a href="https://sites.google.com/view/cloth-sim2real-benchmark"
                  >a recent bimanual cloth manipulation bechmark</a
                >, the cloth physics of these simulators is inaccurate. Among
                all the simulators tested in the benchmark,
                <a href="https://mujoco.org/">MuJoCo</a> is chosen as the one
                achieving closer dynamics. Even if the pick and place positions
                are perfect, the success metric also considers errors from the
                manipulator. While simulators allow the grasp mesh vertices and
                hence achieve an ideal grasp, deploying on real robots
                introduces additional errors due to incorrect grasps or
                workspace limitations. This can be seen in some examples in
                simulation, and we include one of them below:
              </p>
              <figure>
                <div class="columns is-vcentered">
                  <div class="column is-one-quarter">
                    <img
                      src="./media/grasping_failure/00017_Trousers_000004_000180_Fold the trousers, orientating from the bottom right towards the top left._True.png"
                    />
                  </div>
                  <div class="column is-one-quarter">
                    <img src="./media/grasping_failure/00000.png" />
                  </div>
                  <div class="column is-one-quarter">
                    <img src="./media/grasping_failure/00050.png" />
                  </div>
                  <div class="column is-one-quarter">
                    <img src="./media/grasping_failure/00100.png" />
                  </div>
                </div>
                <figcaption>
                  <b>Grasping failure: </b>Example of an instance in which the
                  root of the failure is on the motion primitive. In this case,
                  the language instruction is
                  <code
                    >Fold the trousers, orientating from the bottom right
                    towards the top left.</code
                  >. BiFold produces satisfactory pick and place locations
                  (left), but the actions of the primitive (from second image
                  onwards) fail to grasp the lower layer due to hard-coded
                  action to approach the cloth and pick it up.
                </figcaption>
              </figure>
              <p>
                A natural next step could be to not rely on primitives and
                directly predict robot actions, which could help improve the
                grasp and evaluation of real robots. To do that we could use a
                <a href="https://diffusion-policy.cs.columbia.edu/"
                  >diffusion policy</a
                >
                and
                <a href="https://yusufma03.github.io/projects/hdp/"
                  >condition it with the predicted pick and place actions</a
                >.
              </p>
              <p>
                In this work, we also focus on folding subactions instead of an
                end-to-end fold. However, performing a fold would amount to
                either following a predefined list of ordered instructions or
                using a
                <a href="https://voxposer.github.io/"
                  >LLM-based planner to break down the task into steps </a
                >.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <footer class="footer">
      <!-- <div class="container"> -->
      <!--   <div class="content has-text-centered"> -->
      <!--     <a class="icon-link" -->
      <!--        href="./static/videos/nerfies_paper.pdf"> -->
      <!--       <i class="fas fa-file-pdf"></i> -->
      <!--     </a> -->
      <!--     <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled> -->
      <!--       <i class="fab fa-github"></i> -->
      <!--     </a> -->
      <!--   </div> -->
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a
              <a
                rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/"
                >Creative Commons Attribution-ShareAlike 4.0 International
                License</a
              >
              and borrowed from
              <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a
              >.
            </p>
          </div>
        </div>
      </div>
    </footer>
    <script src="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.3/dist/js/bulma-carousel.min.js"></script>
    <script>
      bulmaCarousel.attach("#unimanual-carousel", {
        slidesToScroll: 1,
        slidesToShow: 4,
        autoplay: true,
        autoplaySpeed: 1500,
      });
      bulmaCarousel.attach("#bimanual-dataset-carousel", {
        slidesToScroll: 1,
        slidesToShow: 4,
        autoplay: true,
        autoplaySpeed: 1500,
      });
      bulmaCarousel.attach("#bimanual-softgym-carousel", {
        slidesToScroll: 1,
        slidesToShow: 4,
        autoplay: true,
        autoplaySpeed: 1500,
      });
      bulmaCarousel.attach("#bimanual-real-carousel", {
        slidesToScroll: 1,
        slidesToShow: 4,
        autoplay: true,
        autoplaySpeed: 2000,
      });
    </script>
    <script>
      document.querySelectorAll("pre").forEach(function (codeBlock) {
        var button = document.createElement("button");
        button.className = "copy-code-button";
        button.type = "button";
        var s = codeBlock.innerText;
        button.setAttribute("data-clipboard-text", s);
        button.innerText = "Copy";
        // var pre = codeBlock.parentNode;
        codeBlock.classList.add("prettyprint");
        // pre.parentNode.insertBefore(button, pre);
        codeBlock.appendChild(button);
      });

      var clipboard = new ClipboardJS(".copy-code-button");

      clipboard.on("success", function (e) {
        console.info("Action:", e.action);
        console.info("Text:", e.text);
        console.info("Trigger:", e.trigger);
        e.trigger.textContent = "Copied!";
        window.setTimeout(function () {
          e.trigger.textContent = "Copy";
        }, 2000);
        e.clearSelection();
      });

      clipboard.on("error", function (e) {
        console.error("Action:", e.action);
        console.error("Trigger:", e.trigger);
        e.trigger.textContent = "Error Copying";
        window.setTimeout(function () {
          e.trigger.textContent = "Copy";
        }, 2000);
        e.clearSelection();
      });
      //]]>
    </script>
  </body>
</html>
